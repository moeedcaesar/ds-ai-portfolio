{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406d419f-59e4-484c-9bd6-297b9005d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9338ff7-3180-46fb-af4c-0db994d037fb",
   "metadata": {},
   "source": [
    "## Loading Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255abee7-c1d8-4675-94db-02a414c0bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3998edf1-c322-4348-aa8a-e9ffa12ba251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fd0d5-5cf4-4bdd-bd00-eaf96fd2efde",
   "metadata": {},
   "source": [
    "## Converting data into X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6d77b4-2d16-436a-adfd-3414dbf24e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af783890-3598-48b8-8963-aa4da8bbc818",
   "metadata": {},
   "source": [
    "## Standard Scaling of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2596a15b-c2c8-47a6-a8c9-b0c21ee34d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a554f2-638c-4577-a3f8-56feadf43134",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X) # use to trnasform them into standar scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ad3f8-cfbe-48c9-bd41-40cdd2d56086",
   "metadata": {},
   "source": [
    "## Conversion of data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ada9382-0cd1-4794-94f4-971cb4355140",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32) # converting them into tensor format\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe0e79-8f08-4a0d-9845-70988ad4b360",
   "metadata": {},
   "source": [
    "## Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6954fb81-caff-4015-8876-050c47fb44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dfcbf2a-9e03-429a-a73b-98428d612f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X_train, y_train) # it is a requirement of the syntex for the model\n",
    "test_ds = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e6a363-63ae-4e4c-8265-4db3263ee1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=16) # batch size should not be very big nor very small i.e. 8, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbe53c51-f382-4d7d-89d7-c916600c53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AnnModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 3)\n",
    "\n",
    "    # defining the flow of the data\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.Relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3120060b-2cbf-4e88-a149-8e050010559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c86841bb-eff7-40fc-845f-b966f948182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "citerion = nn.CrossEntropyLoss() # calculates the difference\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # updates the weights and biases at given rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc7b5de6-ce20-4742-bcea-531c9baf07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50 # one complete cycle through the entire training dataset during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7140eced-ca51-4c1e-b2f0-203e6add5ad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 8.768027305603027\n",
      "Epoch: 1, Loss: 6.200192451477051\n",
      "Epoch: 2, Loss: 4.435360133647919\n",
      "Epoch: 3, Loss: 3.386374995112419\n",
      "Epoch: 4, Loss: 2.6338627636432648\n",
      "Epoch: 5, Loss: 2.3777518421411514\n",
      "Epoch: 6, Loss: 1.9262157529592514\n",
      "Epoch: 7, Loss: 1.6659545302391052\n",
      "Epoch: 8, Loss: 1.5762218236923218\n",
      "Epoch: 9, Loss: 1.4107603654265404\n",
      "Epoch: 10, Loss: 1.1518041044473648\n",
      "Epoch: 11, Loss: 1.047946721315384\n",
      "Epoch: 12, Loss: 0.9335503093898296\n",
      "Epoch: 13, Loss: 0.8531984426081181\n",
      "Epoch: 14, Loss: 0.8575781546533108\n",
      "Epoch: 15, Loss: 0.859555434435606\n",
      "Epoch: 16, Loss: 0.6759940981864929\n",
      "Epoch: 17, Loss: 0.6538369655609131\n",
      "Epoch: 18, Loss: 0.6243088617920876\n",
      "Epoch: 19, Loss: 0.6190301962196827\n",
      "Epoch: 20, Loss: 0.6120967287570238\n",
      "Epoch: 21, Loss: 0.5645208433270454\n",
      "Epoch: 22, Loss: 0.5321470461785793\n",
      "Epoch: 23, Loss: 0.5863674003630877\n",
      "Epoch: 24, Loss: 0.5204787012189627\n",
      "Epoch: 25, Loss: 0.5006831707432866\n",
      "Epoch: 26, Loss: 0.4735823282971978\n",
      "Epoch: 27, Loss: 0.4616673165000975\n",
      "Epoch: 28, Loss: 0.5022254511713982\n",
      "Epoch: 29, Loss: 0.5914995200000703\n",
      "Epoch: 30, Loss: 0.4745346154086292\n",
      "Epoch: 31, Loss: 0.4766833786852658\n",
      "Epoch: 32, Loss: 0.5967497434467077\n",
      "Epoch: 33, Loss: 0.42336627608165145\n",
      "Epoch: 34, Loss: 0.4760614731349051\n",
      "Epoch: 35, Loss: 0.5079487701877952\n",
      "Epoch: 36, Loss: 0.45161570562049747\n",
      "Epoch: 37, Loss: 0.41333267954178154\n",
      "Epoch: 38, Loss: 0.4146090252324939\n",
      "Epoch: 39, Loss: 0.4283070219680667\n",
      "Epoch: 40, Loss: 0.4636435778811574\n",
      "Epoch: 41, Loss: 0.44504672149196267\n",
      "Epoch: 42, Loss: 0.40553522761911154\n",
      "Epoch: 43, Loss: 0.4529740447178483\n",
      "Epoch: 44, Loss: 0.5407939394935966\n",
      "Epoch: 45, Loss: 0.40705591975711286\n",
      "Epoch: 46, Loss: 0.47465354134328663\n",
      "Epoch: 47, Loss: 0.4170380108989775\n",
      "Epoch: 48, Loss: 0.5257808021269739\n",
      "Epoch: 49, Loss: 0.4085174663923681\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad() # optimizer changes weight and bias at given rate\n",
    "        outputs = model(batch_x) # this line applies model on batch_x (features) and store output in variable 'output' \n",
    "        loss = citerion(outputs, batch_y) # Forward Propogation\n",
    "        loss.backward() # Backward Propogation\n",
    "        optimizer.step() # changes the weights and bias according to learning\n",
    "        # by this point the modle is completed and trained the next lines are for personal evaluation\n",
    "        total_loss +=loss.item()\n",
    "    print(f\"Epoch: {epoch}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1c08b2f-1349-4c6d-a280-a0f4bb146da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:     tensor([1, 2, 1, 2, 1, 1, 0, 1, 1, 2, 0, 1, 2, 0, 1, 1])\n",
      "Actual Results: tensor([1, 2, 1, 2, 2, 1, 0, 1, 1, 2, 0, 1, 2, 0, 1, 1])\n",
      "Prediction:     tensor([0, 1, 2, 2, 0, 2, 1, 1, 2, 1, 1, 1, 0, 1])\n",
      "Actual Results: tensor([0, 1, 2, 2, 0, 2, 1, 1, 2, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # this lines tells: don't change anything the model is already trained\n",
    "    lst = []\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x) # the maximum number of output is the most likely prediction\n",
    "        _, prediction = torch.max(outputs, 1) # this function gives the position of maximum output i.e. 0, 1, 2\n",
    "        print(\"Prediction:    \", prediction)\n",
    "        print(\"Actual Results:\", batch_y) # now we are comparing the model predictions with actual results\n",
    "        lst.append(prediction) # a list containing all batches of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13afe350-31f1-4f9a-86e8-aabd15df9974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2, 1, 2, 1, 1, 0, 1, 1, 2, 0, 1, 2, 0, 1, 1]),\n",
       " tensor([0, 1, 2, 2, 0, 2, 1, 1, 2, 1, 1, 1, 0, 1])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d633eac7-ffb8-4d71-800d-7c14f3ae36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versicolor\n",
      "Virginica\n",
      "Versicolor\n",
      "Virginica\n",
      "Versicolor\n",
      "Versicolor\n",
      "Setosa\n",
      "Versicolor\n",
      "Versicolor\n",
      "Virginica\n",
      "Setosa\n",
      "Versicolor\n",
      "Virginica\n",
      "Setosa\n",
      "Versicolor\n",
      "Versicolor\n"
     ]
    }
   ],
   "source": [
    "# now we are changing the predictions with actual names at given positions by manually looking at them\n",
    "for i in lst[0]:\n",
    "    if i.item() == 0:\n",
    "        print(\"Setosa\")\n",
    "    elif i.item() == 1:\n",
    "        print(\"Versicolor\")\n",
    "    else:\n",
    "        print(\"Virginica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b83da29-13b6-41c5-a726-28396dcd8029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target_names'] # this line shows the names of outputs and their positions that we used in above code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
